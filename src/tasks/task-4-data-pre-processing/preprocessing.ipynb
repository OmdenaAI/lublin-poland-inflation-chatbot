{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a6e7fb1e-b142-48fe-bf55-26adb1885cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "import itertools\n",
    "import re\n",
    "import warnings\n",
    "#pip install emoji\n",
    "#nltk.download('stopwords')\n",
    "import emoji\n",
    "import nltk\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "#import en_core_web_sm\n",
    "#nlp=en_core_web_sm.load()\n",
    "#nlp = spacy.load('en_core_web_sm')\n",
    "import string\n",
    "\n",
    "# English and french punctuations. If you think that ? is useful, remove it from the list of punctuations\n",
    "PUNCTUATIONS = '''!()-[]{};:'\"\\\\,<>./?@#$€£₭฿¥%^&*_~'''\n",
    "\n",
    "# list of french stop words\n",
    "STOPWORDS = nltk.corpus.stopwords.words(\"english\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fe2c2377-6b45-4919-bec2-c77312c4f580",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog run cat jump fox watch laugh\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# you can choose to keep basic emoticons in your text if you think it will help in your classification training\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           u\"\\U0001f932\"\n",
    "                           u\"\\u200f\"\n",
    "                           u\"\\U0001F914\"\n",
    "                           u\"\\U0001F923\"\n",
    "                           u\"\\u200D\"\n",
    "                           u\"\\u202c\"\n",
    "                           u\"\\u2069\"\n",
    "                           u\"\\u2066\"\n",
    "                           u\"\\U0001F926\"\n",
    "                           u\"\\U0001F917\"\n",
    "                           u\"\\U0001f928\"\n",
    "                           u\"\\t\"\n",
    "                           u\"\\u200e\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "\n",
    "def remove_urls(text):\n",
    "    # Remove any string that starts with either http:// , https:// , ftp:// or www. plus\n",
    "    # any combination of non white space characters.\n",
    "    text = re.sub('http[s]?://\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub('ftp?://\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub('www.\\S+', '', text, flags=re.MULTILINE)\n",
    "    return text\n",
    "\n",
    "\n",
    "# form a string from a list of tokens\n",
    "def join_tokens(tokens=[], sep=\" \"):\n",
    "    return sep.join(tokens)\n",
    "\n",
    "\n",
    "# remove emojis from a string\n",
    "def remove_emoji(text):\n",
    "    return emoji_pattern.sub(r' ', text)\n",
    "\n",
    "\n",
    "# remove punctuations from the string\n",
    "def remove_punctuations(text):\n",
    "    translator = str.maketrans('', '', PUNCTUATIONS)\n",
    "    return text.translate(translator)\n",
    "\n",
    "\n",
    "def join_tokens(tokens, sep=\" \"):\n",
    "    return sep.join(tokens)\n",
    "\n",
    "\n",
    "def remove_stopwords_from_String(text):\n",
    "    tokens = text.split()\n",
    "    return join_tokens([i for i in tokens if i.lower() not in STOPWORDS], ' ')\n",
    "\n",
    "\n",
    "# if a token is a number it is removed from the list of tokens\n",
    "def remove_digits(text):\n",
    "    text = join_tokens([w for w in text.split(\" \") if not w.isdigit()])\n",
    "    return text\n",
    "def lower_case(text):\n",
    "    text= ' '.join([i.lower() for i in text.split()])\n",
    "    return text\n",
    "\n",
    "def remove_tab_newlines(text):\n",
    "    return re.sub('\\s+',' ',text)\n",
    "\n",
    "# remove any digit in the text\n",
    "def remove_all_numbers(text):\n",
    "    return re.sub(r'\\d+', ' ', text)\n",
    "\n",
    "def remove_contrction(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "def remove_extra_whitespaces(text):\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    doc = nlp(text)\n",
    "    #tokens = doc.split()\n",
    "    lemmatized_sentence = \" \".join([token.lemma_ for token in doc])\n",
    "    return lemmatized_sentence\n",
    "\n",
    "#Usually lemmatization gives better results than stemming\n",
    "def stem(tokens):\n",
    "    stemmer = FrenchStemmer()\n",
    "\n",
    "    for item in tokens:\n",
    "        stems.append(stemmer.stem(item))\n",
    "\n",
    "    return stems\n",
    "\n",
    "\n",
    "\n",
    "# functions can be added or removed\n",
    "def preproc(data=\"\"):\n",
    "    data = str(data)\n",
    "    text = remove_urls(data)\n",
    "    text = remove_emoji(text)\n",
    "    text = remove_contrction(text)\n",
    "    text = remove_punctuations(text)\n",
    "    text = remove_stopwords_from_String(text)\n",
    "    text = remove_tab_newlines(text)\n",
    "    text = remove_digits(text)\n",
    "    text = lemmatize_text(text)\n",
    "    text = lower_case(text)\n",
    "    text = remove_extra_whitespaces(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "print(preproc(\"dogs running after cats jumping . The fox was watching and laughing\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2c4ed0-9670-4d48-ae6b-b1d36a53148c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
